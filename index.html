---
layout: home
---

<div class="sechighlight">
<div class="container sec">
	<h2>Description</h2>
	<div id="coursedesc">
	<p>This course studies learning visual representations for common computer vision tasks including matching, retrieval, classification, and object detection. Related problems are discussed including indexing, nearest neighbor search, clustering, and dimensionality reduction. The course discusses well-known methods from low-level description to intermediate representation, and their dependence on the end task. It then studies a data-driven approach where the entire pipeline is optimized jointly in a supervised fashion, according to a task-dependent objective. Deep learning models are studied in detail and interpreted in connection to conventional models. The focus of the course is on recent, state of the art methods and large scale applications.</p>

	<p>This course is part of master program <a href="http://master.irisa.fr/">Research in Computer Science (SIF)</a> of <a href="https://www.univ-rennes1.fr/">University of Rennes 1</a>.</p>
	</div>
</div>
</div>


<div class="container sec">
  <div class="row">
    <div class="col-md-3">
      <h2>Instructor</h2>
      <a href="http://image.ntua.gr/iva/iavr/">Yannis Avrithis</a>
    </div>
    <div class="col-md-3">
      <h2>Discussions</h2>
      <a href="https://piazza.com/inria.fr/fall2017/dlv">Piazza</a>
    </div>
    <div class="col-md-3">
      <h2>Class</h2>
      Monday and Wednesday <br>
      16:15 to 18:15pm <br>
      Room B02B-E110 (23) <br>
    </div>
    <div class="col-md-3">
      <h2>Evaluation</h2>
      Oral presentations: 50%<br>
      Written exam: 50%<br>
    </div>
  </div>
  </div>
</div>


<div class="sechighlight">
<div class="container sec">
  <h2>Planning and Syllabus</h2>
</div>
</div>

<div class="container sec">
	<table class="table">
		<tr class="active">
			<th>Event</th><th>Date</th><th>Description</th><th>Material</th>
		</tr>
		<tr>
			<td>Lecture-1</td>
			<td>Monday<br>Nov 20</td>
			<td>
				<b>Introduction</b> <br>
				Research field. Neuroscience, computer vision and machine learning background. Modern deep learning. About this course.
			</td>
			<td>
				<a href='slides/intro.pdf'>[slides]</a>
			</td>
		</tr>
		<tr>
			<td>Lecture-2</td>
			<td>Wednesday<br>Nov 22</td>
			<td>
				<b>Representation</b> <br>
				Global/local visual descriptors, dense/sparse representation, feature detectors. Encoding/pooling, vocabularies, bag-of-words. Match kernels, embedding, Fisher vectors, VLAD.
			</td>
			<td>
				<a href='slides/repr.pdf'>[slides]</a>
			</td>
		</tr>
		<tr>
			<td>Lecture-3</td>
			<td>Monday<br>Nov 27</td>
			<td>
				<b>Matching</b> <br>
				Spatial matching, geometric models, RANSAC, Hough transform. Pyramid matching, spatial and Hough pyramids. Object detection, subwindow search, Hough model, deformable part model.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-4</td>
			<td>Monday<br>Dec 4</td>
			<td>
				<b>Indexing</b> <br>
				Clustering, dimensionality reduction, density estimation, nearest neighbor search. Tree-based methods, hashing, product quantization. Inverted index and multi-index.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-5</td>
			<td>Wednesday<br>Dec 8</td>
			<td>
				<b>Learning</b> <br>
				Naive Bayes, nearest neighbor classification. Regression, classification. Logistic regression, support vector machines, neural networks. Activation functions, loss functions, gradient descent.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-6</td>
			<td>Wednesday<br>Dec 12</td>
			<td>
				<b>Differentiation</b> <br>
				Computational graphs, back-propagation, automatic differentiation.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-7</td>
			<td>Monday<br>Dec 18</td>
			<td>
				<b>Convolution</b> <br>
				Pooling, strided convolution, dilated convolution. Convolutional networks. Deconvolution, fully convolutional networks.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-8</td>
			<td>Wednesday<br>Jan 10</td>
			<td>
				<b>Optimization</b> <br>
				Parameter initialization, data-dependent initialization, normalization, regularization. Optimization methods, second-order methods, Hessian-free methods.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr>
			<td>Lecture-9</td>
			<td>Monday<br>Jan 15</td>
			<td>
				<b>Detection</b> <br>
				Class-agnostic region proposals, bounding box regression, non-maxima suppression, part-based models, spatial transformers, attention networks.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr class="warning">
			<td>Evaluation-1</td>
			<td>Wednesday<br>Jan 17</td>
			<td>
				<b>Oral presentations</b> <br>
			</td>
			<td>
				<a href=''></a>
			</td>
		</tr>
		<tr>
			<td>Lecture-10</td>
			<td>Monday<br>Jan 22</td>
			<td>
				<b>Retrieval</b> <br>
				Siamese, triplet, and batch-wise loss functions. Embedding, pooling, dimensionality reduction and manifold learning. Partial matching, spatial matching, quantization, diffusion.
			</td>
			<td>
				[slides]
			</td>
		</tr>
		<tr class="warning">
			<td>Evaluation-2</td>
			<td>Wednesday<br>Jan 24</td>
			<td>
				<b>Written exam</b> <br>
			</td>
			<td>
				<a href=''></a>
			</td>
		</tr>
	</table>
</div>


<div class="container sec">
	<h2>Prerequisites</h2> <br>
	Basic knowledge of Linear Algebra, Calculus, Probabilities, Machine Learning, Python, C++.
</div>


<!--
<h3>Conventional methods</h3>
<ul>
	<li>Global and local visual descriptors, dense and sparse representation, feature detectors. Encoding and pooling methods, vocabularies, bag-of-words. Match kernels, high-dimensional embedding, Fisher vectors, vectors of locally aggregated descriptors.</li>
	<li>Spatial matching, geometric models, RANSAC, Hough transform. Pyramid matching, spatial and Hough pyramids. Object localization and detection, subwindow search, constellation model, Hough model, deformable part model.</li>
	<li>Indexing and approximate nearest neighbor search. Tree-based methods, inverted index and multi-index. Hashing, product quantization, optimized methods. Clustering, dimensionality reduction, density estimation, expectation-maximization.</li>
	<li>Naive Bayes and nearest neighbor classification. Linear regression and classification, logistic regression, support vector machines, neural networks. Activation functions and loss functions, stochastic gradient descent.</li>
</ul>

<h3>Deep learning approach</h3>
<ul>
	<li>Back-propagation, computational graphs, automatic differentiation, Jacobian and Hessian calculation.</li>
	<li>Convolution, pooling, strided convolution, dilated convolution. Convolutional neural networks, deep learning. Deconvolution, fully convolutional networks. Efficient implementations.</li>
	<li>Parameter initialization, data-dependent initialization, normalization, regularization. Optimization methods, second-order methods, Hessian-free methods.</li>
	<li>Convolutional networks for object localization and detection. Class-agnostic region proposals, bounding box regression, non-maxima suppression, part-based models, spatial transformers, attention networks.</li>
	<li>Convolutional networks for image retrieval. Siamese, triplet, and batch-wise loss functions. Embedding, pooling, dimensionality reduction and manifold learning. Region proposals, partial matching, spatial matching, quantization and diffusion.</li>
	<li>Recurrent networks, context modeling. Residual networks, architecture learning. Network visualization, image synthesis, adversarial learning. Transfer learning, weakly supervised, semi-supervised and unsupervised learning.</li>
</ul>
-->