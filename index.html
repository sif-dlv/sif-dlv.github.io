---
layout: home
---

<h3>Conventional methods</h3>

<div class="container sec">
<table class="table">
	<tr class="active">
		<th>Event</th><th>Date</th><th>Description</th><th>Material</th>
	</tr>
	<tr>
		<td>Lecture 1</td>
		<td>Monday<br>Nov 20</td>
		<td>
			<b>Introduction</b> <br>
			Research field. Neuroscience, computer vision and machine learning background. Modern deep learning. About this course.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 2</td>
		<td>Wednesday<br>Nov 22</td>
		<td>
			<b>Representation</b> <br>
			Global/local visual descriptors, dense/sparse representation, feature detectors. Eencoding/pooling, vocabularies, bag-of-words. Match kernels, embedding, Fisher vectors, VLAD.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 3</td>
		<td>Monday<br>Nov 27</td>
		<td>
			<b>Matching</b> <br>
			Spatial matching, geometric models, RANSAC, Hough transform. Pyramid matching, spatial and Hough pyramids. Object detection, subwindow search, Hough model, deformable part model.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 4</td>
		<td>Monday<br>Dec 4</td>
		<td>
			<b>Indexing</b> <br>
			Clustering, dimensionality reduction, density estimation, nearest neighbor search. Tree-based methods, hashing, product quantization. Inverted index and multi-index.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 5</td>
		<td>Wednesday<br>Dec 8</td>
		<td>
			<b>Learning</b> <br>
			Naive Bayes, nearest neighbor classification. Regression, classification. Logistic regression, support vector machines, neural networks. Activation functions, loss functions, gradient descent.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
</table>
</div>


<h3>Deep learning approach</h3>

<div class="container sec">
<table class="table">
	<tr class="active">
		<th>Event</th><th>Date</th><th>Description</th><th>Material</th>
	</tr>
	<tr>
		<td>Lecture 6</td>
		<td>Wednesday<br>Dec 12</td>
		<td>
			<b>Differentiation</b> <br>
			Computational graphs, back-propagation, automatic differentiation.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 7</td>
		<td>Monday<br>Dec 18</td>
		<td>
			<b>Convolution</b> <br>
			Pooling, strided convolution, dilated convolution. Convolutional networks. Deconvolution, fully convolutional networks.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 8</td>
		<td>Wednesday<br>Jan 10</td>
		<td>
			<b>Optimization</b> <br>
			Parameter initialization, data-dependent initialization, normalization, regularization. Optimization methods, second-order methods, Hessian-free methods.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Lecture 9</td>
		<td>Monday<br>Jan 15</td>
		<td>
			<b>Detection</b> <br>
			Class-agnostic region proposals, bounding box regression, non-maxima suppression, part-based models, spatial transformers, attention networks.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Evaluation 1</td>
		<td>Wednesday<br>Jan 17</td>
		<td>
			<b>Oral presentations</b> <br>
		</td>
		<td>
			<a href=''></a>
		</td>
	</tr>
	<tr>
		<td>Lecture 10</td>
		<td>Monday<br>Jan 22</td>
		<td>
			<b>Retrieval</b> <br>
			Siamese, triplet, and batch-wise loss functions. Embedding, pooling, dimensionality reduction and manifold learning. Partial matching, spatial matching, quantization, diffusion.
		</td>
		<td>
			<a href=''>[slides]</a>
		</td>
	</tr>
	<tr>
		<td>Evaluation 2</td>
		<td>Wednesday<br>Jan 24</td>
		<td>
			<b>Written exam</b> <br>
		</td>
		<td>
			<a href=''></a>
		</td>
	</tr>
</table>
</div>


<h3>Conventional methods</h3>
<ul>
	<li>Global and local visual descriptors, dense and sparse representation, feature detectors. Encoding and pooling methods, vocabularies, bag-of-words. Match kernels, high-dimensional embedding, Fisher vectors, vectors of locally aggregated descriptors.</li>
	<li>Spatial matching, geometric models, RANSAC, Hough transform. Pyramid matching, spatial and Hough pyramids. Object localization and detection, subwindow search, constellation model, Hough model, deformable part model.</li>
	<li>Indexing and approximate nearest neighbor search. Tree-based methods, inverted index and multi-index. Hashing, product quantization, optimized methods. Clustering, dimensionality reduction, density estimation, expectation-maximization.</li>
	<li>Naive Bayes and nearest neighbor classification. Linear regression and classification, logistic regression, support vector machines, neural networks. Activation functions and loss functions, stochastic gradient descent.</li>
</ul>

<h3>Deep learning approach</h3>
<ul>
	<li>Back-propagation, computational graphs, automatic differentiation, Jacobian and Hessian calculation.</li>
	<li>Convolution, pooling, strided convolution, dilated convolution. Convolutional neural networks, deep learning. Deconvolution, fully convolutional networks. Efficient implementations.</li>
	<li>Parameter initialization, data-dependent initialization, normalization, regularization. Optimization methods, second-order methods, Hessian-free methods.</li>
	<li>Convolutional networks for object localization and detection. Class-agnostic region proposals, bounding box regression, non-maxima suppression, part-based models, spatial transformers, attention networks.</li>
	<li>Convolutional networks for image retrieval. Siamese, triplet, and batch-wise loss functions. Embedding, pooling, dimensionality reduction and manifold learning. Region proposals, partial matching, spatial matching, quantization and diffusion.</li>
	<li>Recurrent networks, context modeling. Residual networks, architecture learning. Network visualization, image synthesis, adversarial learning. Transfer learning, weakly supervised, semi-supervised and unsupervised learning.</li>
</ul>